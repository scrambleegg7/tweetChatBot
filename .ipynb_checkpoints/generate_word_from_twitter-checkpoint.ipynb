{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "import re\n",
    "import mojimoji\n",
    "\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "\n",
    "import codecs\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "import unicodedata\n",
    "import urllib.request as urllib2\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pickle\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt = platform.system()\n",
    "\n",
    "#mecab = MeCab.Tagger ('-Ochasen')\n",
    "if plt == \"Linux\":\n",
    "    tagger = MeCab.Tagger('-d /usr/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "if plt == \"Windows\":\n",
    "    tagger = MeCab.Tagger(\"-r C:\\MeCab\\etc\\mecabrc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file names :  ['tweet/tweet2020-07-22.txt', 'tweet/tweet2020-07-19.txt', 'tweet/tweet_Go To_2020-07-24.txt', 'tweet/tweet2020-07-21.txt']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "file_list = glob.glob('tweet/*')\n",
    "print(\"file names : \" , file_list)\n",
    "print(len(file_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = file_list[0]\n",
    "colsnames = [ 'c{0:02d}'.format(i) for i in range(10) ]\n",
    "with codecs.open(file, \"r\", \"utf8\", \"ignore\") as f:\n",
    "    #df_out = pd.read_csv(f, colsnames)\n",
    "    df1 = csv.reader(f)\n",
    "    data = [v  for v in df1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['REQ:いつもの見出ししか読まない人らが470万のところまでたどり着けるわけがないw'],\n",
       " ['RES:ニカイガー、のとこまで読んだ(*・ω・)ノ俺はGO TO キャンペーン推進派やから、二階さん頑張れ！(*ﾟ▽ﾟ)ﾉって感じ二階さん地元の和歌山旅行行こかな'],\n",
       " ['REQ:こんばんは。今日も一日お疲れ様でした。外来リハビリ一日目終了。これだけ親指や付け根を解してもまだまだ十分の一程度との事です。明日もしっかりやってもらいますよ。いいね返しあまり出来ずに申し訳ございません。フォローバッ… '],\n",
       " ['RES:こんばんは。️今日もお疲れ様でした。連日のリハビリ、翌日に支障をきたさないように注意してくださいね美ヶ原高原、綺麗ですね。明日からGo Toとやらがはじまるようですが、みなさんが新型コ… '],\n",
       " ['REQ:人吉市の避難所では、『火の国の湯』と名付けられたところで、自衛隊が入浴支援を行っています。発災から一週間余り、被災者の皆さんのため、救助活動のほか、災害廃棄物の搬出支援など生活支援に全力で当たっている彼らを本当に誇りに思います。 '],\n",
       " ['RES:さっさと二階は切り捨てなさい。「足を引っ張るだけのゴミ」です。ネットを使いこなせない老害。二階は己の評判も理解していない。文春報道「Go Toキャンペーン受託団体が二階幹事長らに4200万円献金」'],\n",
       " ['REQ:見てるだけで怖い福岡身近すぎて他県ごとじゃない'],\n",
       " ['RES:住んでる市もポツポツ罹患者出て来てて怖いです。go toどころじゃないよ～'],\n",
       " ['REQ:同時に、多くの農林漁業者、中小・小規模事業者の皆さんが、コロナウイルスで大きな影響を受ける中で、今回の災害が発生し、事業再開への気力も失いかねない厳しい状況にある。そうした声もうかがいました。'],\n",
       " ['RES:さっさと二階は切り捨てなさい。「足を引っ張るだけのゴミ」です。ネットを使いこなせない老害。二階は己の評判も理解していない。文春報道「Go Toキャンペーン受託団体が二階幹事長らに4200万円献金」'],\n",
       " ['REQ:【']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows : 1094\n"
     ]
    }
   ],
   "source": [
    "f=open(file, 'r' ,encoding=\"utf-8_sig\")\n",
    "df1 = csv.reader(f)\n",
    "data = [ v for v in df1]\n",
    "print('number of rows :', len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short response/ request \n",
      "10 REQ 【\n",
      "short response/ request \n",
      "28 REQ In Japan\n",
      "short response/ request \n",
      "57 RES Go Toとは\n",
      "short response/ request \n",
      "88 REQ こわ\n",
      "short response/ request \n",
      "96 REQ 明日から5連休。\n",
      "short response/ request \n",
      "120 REQ 今夜も\n",
      "short response/ request \n",
      "128 REQ 占っティ\n",
      "short response/ request \n",
      "144 REQ 明太で8即するね？\n",
      "short response/ request \n",
      "178 REQ 【\n",
      "short response/ request \n",
      "209 RES go to 腹減る\n",
      "short response/ request \n",
      "315 RES 正義が動き出した。\n",
      "short response/ request \n",
      "326 REQ 早く宣言しろ\n",
      "short response/ request \n",
      "327 RES Go To 職場ね\n",
      "short response/ request \n",
      "360 REQ 【俳優の \n",
      "short response/ request \n",
      "366 REQ 軟骨だからむり\n",
      "short response/ request \n",
      "396 REQ 【\n",
      "short response/ request \n",
      "400 REQ 【\n",
      "short response/ request \n",
      "418 REQ どうしてですか。？\n",
      "short response/ request \n",
      "452 REQ 本日のフォンチー\n",
      "short response/ request \n",
      "476 REQ そうね…。\n",
      "short response/ request \n",
      "599 RES GO TO やん。\n",
      "short response/ request \n",
      "632 REQ ﾅﾆｺﾚ \n",
      "short response/ request \n",
      "725 RES Go toですか？\n",
      "short response/ request \n",
      "788 REQ とうとうオワタ。 \n",
      "short response/ request \n",
      "812 REQ つし、ま……\n",
      "short response/ request \n",
      "872 REQ 列並び \n",
      "short response/ request \n",
      "934 REQ 貯虎羨まし～。\n",
      "short response/ request \n",
      "973 RES 片山さん\n",
      "short response/ request \n",
      "982 REQ 福岡も。\n",
      "short response/ request \n",
      "1001 RES 石原さん\n",
      "short response/ request \n",
      "1013 RES 山田さん\n",
      "short response/ request \n",
      "1035 RES 鈴木さん\n",
      "short response/ request \n",
      "1045 RES 利権の \n",
      "short response/ request \n",
      "1066 REQ おかえり\n"
     ]
    }
   ],
   "source": [
    "flag = 0\n",
    "pref_idx = 0\n",
    "req_idx = []\n",
    "res_idx = []\n",
    "for idx, d in enumerate(data):\n",
    "\n",
    "    prefix = data[idx][0].split(\":\")[0]\n",
    "    sentence = data[idx][0].split(\":\")[1]\n",
    "    \n",
    "    if len(sentence) < 10:\n",
    "        print(\"short response/ request \")\n",
    "        print(idx,prefix,sentence)\n",
    "        if prefix == \"REQ\":\n",
    "            req_idx.append(idx)\n",
    "        if prefix == \"RES\":\n",
    "            res_idx.append(idx)\n",
    "        \n",
    "    \n",
    "    if prefix == \"REQ\" and idx % 2 == 0:\n",
    "        flag = 1\n",
    "        prev_idx = idx\n",
    "    if prefix == \"RES\" and idx % 2 == 1:\n",
    "        if idx - prev_idx != 1:\n",
    "            print(\"ERROR\", prev_idx, idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "政治家なんか一人もいない方が国民は幸せかも平和ボケなクソ政治家ばかり Go to も失敗やのに説明や責任も取らない利権の金だけは受け取る。まさに穀潰し。\n",
      "うなぎ is absolutely amazing and it's always my go to whenever I have the option. I haven't had it since t… \n",
      "東京都民go toなんちゃら使えないらしいです\n",
      "私の有給休暇は補償してくれるんですか？GO TO キャンペーンノーカウントにしてくれるんですか？旅に出ていいんですか？国土交通省さん\n",
      "この間の感じからして16位以下だと安心できませんね。目指せ11位。Go to Eleven！ \n",
      "タロットは「戦車」（正）順調に進んでいます。なりふり構わずそのままのペースで。仕事→歯車が噛み合うように調子が良い。恋愛→気持ちをストレートに伝える。行動するなら今。すなわち、やるっきゃナイト。『… \n",
      "無理やったら僕のGO TOキャンペーンでキャッシュバックされなかった分の宿代をキャッシュバックしてください\n",
      "Go Toキャンペーン受託団体が二階幹事長らに4200万円献金大スキャンダルではないですか。何故このようなことがまかり通るのでしょうか。\n",
      "Go To 職場ね\n",
      "どんどんコロナ感染拡大してる中、「Go To キャンペーン」と「外出自粛」の同時進行とかもうめちゃくちゃで笑えてくるwで、二階幹事長をはじめ自民党議員が献金を受け取ってたって？さっさとこの老害自民党政権をぶっ壊さなきゃダメだな！\n",
      "Go To 整形外科 じゃん…おだいじに\n",
      "Go Toキャンペーン受託団体が二階幹事長らに4200万円献金 | 文春オンライン \n",
      "Go Toキャンペーン受託団体が二階幹事長らに4200万円献金 | 文春オンライン 、。野郎\n",
      "Go to トラベルキャンペーンでも除外されたし、東京のナンバープレートつけてたら迷惑がられますからね…\n",
      "I want to go to Vietnam. ではTôi muốn đến việt nam. のようです。I wana go to Vietnam.にするとお題のTôi muốn đi việt nam. になります… \n",
      "そんな俺はセルフGo to キャンペーンfor東京の準備中明日と明後日の２日連続都内に遊びに出掛けるのが大変だから「泊まってしまえ！」ってことになったんだけど、泊まるとなるとそれはそれで準備大変(キャンピングカー欲しい※自宅にしたい)\n",
      "いきたい!I want to go!\n",
      "無理にでも旅行させたい理由はこれですね！2F Go to house！！\n",
      "Go Toトラベル使えない都民なので、対馬観光も一考やな。\n",
      "go to キャンペーン開始ですね。\n",
      "（GO TO 兵庫したい）\n",
      "福岡も増えてきてますよね。go toしてる場合じゃない…。\n",
      "今GO  TO 東京してるんだ…\n"
     ]
    }
   ],
   "source": [
    "for req_i in req_idx:\n",
    "    sentence = data[req_i + 1][0].split(\":\")[1]\n",
    "    print(sentence)\n",
    "    data[req_i][0] = \"REQ:\" + sentence\n",
    "\n",
    "for res_i in res_idx:\n",
    "    sentence = data[res_i - 1][0].split(\":\")[1]\n",
    "    data[res_i][0] = \"RES:\" + sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['REQ:【']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RES:政治家なんか一人もいない方が国民は幸せかも平和ボケなクソ政治家ばかり Go to も失敗やのに説明や責任も取らない利権の金だけは受け取る。まさに穀潰し。']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10,\n",
       " 28,\n",
       " 88,\n",
       " 96,\n",
       " 120,\n",
       " 128,\n",
       " 144,\n",
       " 178,\n",
       " 326,\n",
       " 360,\n",
       " 366,\n",
       " 396,\n",
       " 400,\n",
       " 418,\n",
       " 452,\n",
       " 476,\n",
       " 632,\n",
       " 788,\n",
       " 812,\n",
       " 872,\n",
       " 934,\n",
       " 982,\n",
       " 1066]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REQ:【'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RES:さっさと二階は切り捨てなさい。「足を引っ張るだけのゴミ」です。ネットを使いこなせない老害。二階は己の評判も理解していない。文春報道「Go Toキャンペーン受託団体が二階幹事長らに4200万円献金」'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[9][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.permutation(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RES:【Go To Travelキャンペーン概要②】・旅行先で使えるクーポンの本格実施日は9月1日以降で別途お知らせ・クーポンは1枚1000円単位で発行・クーポンのお釣りはなし・1000円未満は四捨五入・クーポンは旅行先都道… \n"
     ]
    }
   ],
   "source": [
    "text = data[r[0]][0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nan(x):\n",
    "    return (x != x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_normalize(cls, s):\n",
    "    pt = re.compile('([{}]+)'.format(cls))\n",
    "\n",
    "    def norm(c):\n",
    "        return unicodedata.normalize('NFKC', c) if pt.match(c) else c\n",
    "\n",
    "    s = ''.join(norm(x) for x in re.split(pt, s))\n",
    "    s = re.sub('－', '-', s)\n",
    "    return s\n",
    "\n",
    "def remove_extra_spaces(s):\n",
    "    s = re.sub('[ 　]+', ' ', s)\n",
    "    blocks = ''.join(('\\u4E00-\\u9FFF',  # CJK UNIFIED IDEOGRAPHS\n",
    "                      '\\u3040-\\u309F',  # HIRAGANA\n",
    "                      '\\u30A0-\\u30FF',  # KATAKANA\n",
    "                      '\\u3000-\\u303F',  # CJK SYMBOLS AND PUNCTUATION\n",
    "                      '\\uFF00-\\uFFEF'   # HALFWIDTH AND FULLWIDTH FORMS\n",
    "                      ))\n",
    "    basic_latin = '\\u0000-\\u007F'\n",
    "\n",
    "    def remove_space_between(cls1, cls2, s):\n",
    "        p = re.compile('([{}]) ([{}])'.format(cls1, cls2))\n",
    "        while p.search(s):\n",
    "            s = p.sub(r'\\1\\2', s)\n",
    "        return s\n",
    "\n",
    "    s = remove_space_between(blocks, blocks, s)\n",
    "    s = remove_space_between(blocks, basic_latin, s)\n",
    "    s = remove_space_between(basic_latin, blocks, s)\n",
    "    return s\n",
    "\n",
    "def normalize_neologd(s):\n",
    "    s = s.strip()\n",
    "    s = unicode_normalize('０-９Ａ-Ｚａ-ｚ｡-ﾟ', s)\n",
    "\n",
    "    def maketrans(f, t):\n",
    "        return {ord(x): ord(y) for x, y in zip(f, t)}\n",
    "\n",
    "    s = re.sub('[˗֊‐‑‒–⁃⁻₋−]+', '-', s)  # normalize hyphens\n",
    "    s = re.sub('[﹣－ｰ—―─━ー]+', 'ー', s)  # normalize choonpus\n",
    "    s = re.sub('[~∼∾〜〰～]', '', s)  # remove tildes\n",
    "    s = s.translate(\n",
    "        maketrans('!\"#$%&\\'()*+,-./:;<=>?@[¥]^_`{|}~｡､･｢｣',\n",
    "              '！”＃＄％＆’（）＊＋，－．／：；＜＝＞？＠［￥］＾＿｀｛｜｝〜。、・「」'))\n",
    "\n",
    "    s = remove_extra_spaces(s)\n",
    "    s = unicode_normalize('！”＃＄％＆’（）＊＋，－．／：；＜＞？＠［￥］＾＿｀｛｜｝〜', s)  # keep ＝,・,「,」\n",
    "    s = re.sub('[’]', '\\'', s)\n",
    "    s = re.sub('[”]', '\"', s)\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-44-f1dfd4835866>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-44-f1dfd4835866>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    new_text = normalize_neologd(\"RES:\"「Go To トラベル」で国交相 高齢者や若者の団体は割引対象外\"というのも若者の感染からの死亡率は低いだろうし金貯めこんでいる高齢者には頑張って長距離旅行して金バラまいてほしい気がするけど。(感染拡大のリスクはあるけど)今の政治はチグハグで納得できんかな。\")\u001b[0m\n\u001b[1;37m                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "new_text = normalize_neologd(\"RES:「Go To トラベル」で国交相 高齢者や若者の団体は割引対象外\"というのも若者の感染からの死亡率は低いだろうし金貯めこんでいる高齢者には頑張って長距離旅行して金バラまいてほしい気がするけど。(感染拡大のリスクはあるけど)今の政治はチグハグで納得できんかな。\")\n",
    "print(\"before : \",text)\n",
    "print(\"after :  \",new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words():\n",
    "    slothlib_path = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "    slothlib_file = urllib2.urlopen(slothlib_path)\n",
    "    slothlib_stopwords = [line.decode(\"utf-8\").strip() for line in slothlib_file]\n",
    "    slothlib_stopwords = [ss for ss in slothlib_stopwords if not ss==u'']\n",
    "    \n",
    "    #print(slothlib_stopwords)\n",
    "    return slothlib_stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def omitChar(text):\n",
    "    text=re.sub('お気に入り', \"\", text)\n",
    "    text=re.sub('まとめ', \"\", text)\n",
    "    #text=re.sub(r'[!-~]', \"\", text)#半角記号,数字,英字\n",
    "    text=re.sub(r'[︰-＠]', \"\", text)#全角記号\n",
    "    text=re.sub('\\n', \" \", text)#改行文字 \n",
    "    \n",
    "    #kigo = '\\\"!\"#$%&'()*+\\,-./;<=>?@[\\]^_`{|}~\"'\n",
    "    kigo = string.punctuation\n",
    "    collon = str.maketrans( '', '',':')\n",
    "    \n",
    "    kigo = kigo.translate(collon)\n",
    "    table = str.maketrans( '', '',kigo)\n",
    "    text = text.translate(table)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sloth_stopwords = stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "def normalize_number(text):\n",
    "    # 連続した数字を0で置換\n",
    "    replaced_text = re.sub(r'\\d+', '0', text)\n",
    "    return replaced_text\n",
    "\n",
    "def decomposition(file, stop_words) :\n",
    "    with codecs.open(file, \"r\", \"utf8\", \"ignore\") as f:\n",
    "\n",
    "        df1 = csv.reader(f)\n",
    "        data = [ v for v in df1]\n",
    "        print('number of rows :', len(data))\n",
    "\n",
    "        parts = []\n",
    "        skip_counts = 0\n",
    "        for i in range(len(data)) :\n",
    "            if len(data[i][0].encode('utf-8')) <= 4096 :\n",
    "\n",
    "                \n",
    "                try:\n",
    "                    after_data =omitChar(data[i][0])\n",
    "                    result = tagger.parse(after_data)\n",
    "                    df_analyzed = pd.read_csv(StringIO(result), delimiter='\\t', names=['単語'] )\n",
    "                except:\n",
    "                    print(\"- \" * 20)\n",
    "                    print()\n",
    "                    print(\"*********          error           ********\")\n",
    "                    print(data[i][0])\n",
    "                    print(\"BEFORE--> \",normalize_neologd(data[i][0]))\n",
    "                    after_data =omitChar(data[i][0])\n",
    "                    print(\"AFTER --> \", after_data)\n",
    "                    #result = tagger.parse( after_data )\n",
    "                    #df_analyzed = pd.read_csv(StringIO(result), delimiter='\\t', names=['単語'] )\n",
    "\n",
    "                    continue\n",
    "\n",
    "                for idx, w in enumerate(df_analyzed[\"単語\"]):\n",
    "                    #print(w.split(\",\")[6])\n",
    "                    if w != \"EOS\" and not is_nan(w):\n",
    "\n",
    "                        #word = w.split(\",\")[6]\n",
    "                        word = df_analyzed.index[idx]\n",
    "                        type = w.split(\",\")[0]\n",
    "                        if not type in (\"名詞\",\"動詞\",\"形容詞\"):\n",
    "                            continue\n",
    "\n",
    "                        if word == \"REQ\" and df_analyzed.index[idx+1] == \":\":\n",
    "                            word = \"REQREQ\"\n",
    "                        if word == \"RES\" and df_analyzed.index[idx+1] == \":\":\n",
    "                            word = \"RESRES\"\n",
    "                            \n",
    "                        word = normalize_number(word)\n",
    "                        word = lower_text(word)\n",
    "                        #print(\"words...%d %s \" % (idx, word)  )\n",
    "                        parts.append( word )\n",
    "                \n",
    "\n",
    "                    else :\n",
    "                        #print(i, ' skip')\n",
    "                        skip_counts += 1\n",
    "                        continue\n",
    "            #for mrph in result:\n",
    "                    \n",
    "            if i % 500 == 0 :\n",
    "                print(\"== \" *20)\n",
    "                print(\"sentence line counts --> \", (i+1)  )\n",
    "                print(\"word counts by MeCab\", len(parts))\n",
    "                print(\"extracted words\", parts[i:i+10])\n",
    "                print(\"Skip counts \", skip_counts)\n",
    "                #print(df_analyzed)\n",
    "\n",
    "    print(\"Total Skip counts \", skip_counts)\n",
    "\n",
    "    print(\"exclude Stop words\")\n",
    "    count_vectorizer = CountVectorizer(stop_words=stop_words,token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\n",
    "    feature_vectors = count_vectorizer.fit_transform(parts)\n",
    "    vocabulary = count_vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "    parts = [p for p in parts if p in vocabulary ]\n",
    "    print(\"parts printing...\")\n",
    "    print(parts[:10])\n",
    "\n",
    "    new_parts = []\n",
    "    re_hiragana = re.compile(r'[あ-んA-Za-z0-9]$')\n",
    "    hiragana_status = [ re_hiragana.fullmatch(w) for w in parts]\n",
    "    for idx, status in enumerate(hiragana_status):\n",
    "        if not status:\n",
    "            new_parts.append(parts[idx])\n",
    "\n",
    "    return new_parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows : 4594\n",
      "== == == == == == == == == == == == == == == == == == == == \n",
      "sentence line counts -->  1\n",
      "word counts by MeCab 13\n",
      "extracted words ['reqreq', ':', 'コロナ', '0', '波', 'いい', '豪雨', 'いい', '今年', 'なっ']\n",
      "Skip counts  1\n",
      "== == == == == == == == == == == == == == == == == == == == \n",
      "sentence line counts -->  501\n",
      "word counts by MeCab 10199\n",
      "extracted words [':', 'go', 'to', '無視', 'すれ', '行き', '放題', 'ん', 'ww', '恩恵']\n",
      "Skip counts  501\n",
      "== == == == == == == == == == == == == == == == == == == == \n",
      "sentence line counts -->  1001\n",
      "word counts by MeCab 19636\n",
      "extracted words [':', 'サイコロ', '個', '振っ', '合計', '偶数', 'go', 'to', 'camp', 'reqreq']\n",
      "Skip counts  1001\n",
      "== == == == == == == == == == == == == == == == == == == == \n",
      "sentence line counts -->  1501\n",
      "word counts by MeCab 28804\n",
      "extracted words ['放送予定', '都内', '0人', '感染', 'ｇｏｔｏ', '見直し', 'ｇｏｔｏ', 'トラベル', '東京', '除外']\n",
      "Skip counts  1501\n",
      "== == == == == == == == == == == == == == == == == == == == \n",
      "sentence line counts -->  2001\n",
      "word counts by MeCab 37802\n",
      "extracted words ['一', '人', '歩き', 'し', '今', 'こと', 'なっ', 'い', 'resres', ':']\n",
      "Skip counts  2001\n",
      "== == == == == == == == == == == == == == == == == == == == \n",
      "sentence line counts -->  2501\n",
      "word counts by MeCab 47486\n",
      "extracted words ['論外', 'resres', ':', '感染対策', '万全', 'し', '上', '国内', '経済', '回す']\n",
      "Skip counts  2501\n",
      "== == == == == == == == == == == == == == == == == == == == \n",
      "sentence line counts -->  3001\n",
      "word counts by MeCab 56977\n",
      "extracted words ['県民', '市民', 'go', 'to', 'キャンペーン', '行っ', 'どうでしょう', '感染', '拡大', 'せ']\n",
      "Skip counts  3001\n",
      "== == == == == == == == == == == == == == == == == == == == \n",
      "sentence line counts -->  3501\n",
      "word counts by MeCab 66254\n",
      "extracted words ['安倍首相', '迷い', '時事通信', 'resres', ':', 'go', 'to', '全国的', '延期', '言う']\n",
      "Skip counts  3501\n",
      "== == == == == == == == == == == == == == == == == == == == \n",
      "sentence line counts -->  4001\n",
      "word counts by MeCab 75124\n",
      "extracted words ['resres', ':', '立憲民主党', '狂い', '過ぎ', 'てる', '0', 'go', 'to', '早く']\n",
      "Skip counts  4001\n",
      "== == == == == == == == == == == == == == == == == == == == \n",
      "sentence line counts -->  4501\n",
      "word counts by MeCab 84623\n",
      "extracted words ['おい', '観光業', 'ハイ', 'シーズン', 'なる', '稼ぎ', '時', '政策', '行う', 'の']\n",
      "Skip counts  4501\n",
      "Total Skip counts  4594\n",
      "exclude Stop words\n",
      "parts printing...\n",
      "['reqreq', 'コロナ', '0', '波', 'いい', '豪雨', 'いい', '今年', 'なっ', 'てる']\n"
     ]
    }
   ],
   "source": [
    "new_parts = decomposition(file,sloth_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reqreq', 'コロナ', '波', 'いい', '豪雨', 'いい', '今年', 'なっ', 'てる', 'しよ', 'よろしくお願いします', 'resres', '最近', '災害', '年々', '多発', 'コロナ', '加わっ', 'go', 'to', 'やっ', 'てる', '場所', 'reqreq', '怖い', '東京', '行っ', '東京', '訪れ', '緊急事態宣言', '解除', '良い', 'ない', 'go', 'to', 'キャンペーン', 'ホント', '信じ', 'られ', 'resres', '東京', '来', '懇談会', '聞い', 'go', 'to', 'キャンペーン', '信じ', 'られ', '大変', 'なる', '見え', 'てる', 'reqreq', '離島', '来島', '自粛要請', 'だし', 'てる', 'コロナ', '落ち着い', '幻想', 'すがる', '取り残さ', 'れる', '梅雨', '落ち着く', '論者', '行っ', 'しばらく', '落ち着か', '危な', '病気', '切り替え', 'いく', 'ない', '島', '観光', '死ん', '言う', '旅行', '楽しみ', 'resres', 'go', 'to', 'travel', '行っ', '非国民', 'られ', '石', '投げ', 'られ', '車', '傷', 'つけ', 'られ', '思う', '被害妄想', 'reqreq', '星新一']\n"
     ]
    }
   ],
   "source": [
    "print(new_parts[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65192"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['手すり', '小さい', 'いびつ', 'オフィス', '私物', 'コンサート', 'what', '一概', '心配', '会議', '初代', '攻撃', 'スポーツ', 'ピンチ', '第二種', '避', '不満', 'ホショウ', '計画通り', 'グルメ', '従来', '管理', 'ha', 'ウンザリ', 'くま', '寝', '気象庁', 'カッコ', 'ﾋｬｯﾊｰ', '尾見', '健作', '解説', '代理', 'プレゼ', 'ホテルマン', '想像', '愛知県民', '政務調査会長', '窓口', 'おもしろい', 'はる', '生', '役割', 'グッズ', 'プチ', '溜まっ', '尊い', '東京新聞', 'to', 'ちゃーん', '定住者', 'ふう', '国籍', '気持ち良く', 'チャン', 'よし', 'みあげ', 'リ', '狂い', '適切', '近けれ', '泊まれる', '富士', '定着', 'あま', '遊ぶ', 'ﾒﾀﾞﾙ', '合言葉', '右寄り', '行く末', '免れ', '遊び', 'とんちんかん', '胡散臭い', 'イニ', '大田区', 'スキンケア', '落ちろ', 'えってん', 'トラ', '立民', 'わたくし', '付け加え', '旅費', 'tattoofountains', '感染症', '青森', '幸せ', '幸い', 'ｗｗｗ', '自由民主党幹事長', '示し', '殴', 'サービス', '浜田', 'じせ', 'クリーン', '大人', '有馬温泉', '風になる']\n"
     ]
    }
   ],
   "source": [
    "print( list(set(new_parts))[:100] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ファイル数 = 1\n",
      "FileName ['list_corpus\\\\list_corpus_0.pickle']\n",
      "file counting -->  0\n",
      "total matrix length :  97302\n",
      "total words: 7569\n"
     ]
    }
   ],
   "source": [
    "file_list = glob.glob('list_corpus/*')\n",
    "print('ファイル数 =',len(file_list))\n",
    "print('FileName', file_list)\n",
    "mat=[]\n",
    "for i in range (0,len(file_list)) :\n",
    "    with open(file_list[i],'rb') as f :\n",
    "        generated_list=pickle.load(f)         #生成リストロード\n",
    "        mat.extend(generated_list)\n",
    "        print(\"file counting --> \",i)\n",
    "        del generated_list\n",
    "\n",
    "\n",
    "mat.append('REQREQ')\n",
    "\n",
    "print(\"total matrix length : \", len(mat))\n",
    "words = sorted(list(set(mat)))\n",
    "cnt = np.zeros(len(words))\n",
    "\n",
    "print('total words:', len(words))\n",
    "word_indices = dict((w, i) for i, w in enumerate(words))    #単語をキーにインデックス検索\n",
    "indices_word = dict((i, w) for i, w in enumerate(words))    #インデックスをキーに単語を検索\n",
    "\n",
    "#単語の出現数をカウント\n",
    "for j in range (0,len(mat)):\n",
    "    cnt[word_indices[mat[j]]] += 1\n",
    "\n",
    "#出現頻度の少ない単語を「UNK」で置き換え\n",
    "words_unk = []                                #未知語一覧\n",
    "\n",
    "for k in range(0,len(words)):\n",
    "    if cnt[k] <= 10 :\n",
    "        words_unk.append(words[k])\n",
    "        words[k] = 'UNK'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'resres'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_cnt = np.argmax(cnt)\n",
    "indices_word[max_cnt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reqreq', 'コロナ', '波', 'いい', '豪雨', 'いい', '今年', 'なっ', 'てる', 'しよ', 'よろしくお願いします', 'resres', '最近', '災害', '多発', 'コロナ', 'やっ', 'てる', '場所', 'reqreq', '怖い', '東京', '行っ', '東京', '訪れ', '緊急事態宣言', '解除', '良い', 'ない', 'キャンペーン', 'ホント', '信じ', 'られ', 'resres', '東京', '来', '聞い', 'キャンペーン', '信じ', 'られ', '大変', 'なる', '見え', 'てる', 'reqreq', '離島', '来島', '自粛要請', 'だし', 'てる', 'コロナ', '落ち着い', '幻想', '取り残さ', 'れる', '梅雨', '落ち着く', '行っ', '落ち着か', '危な', '病気', '切り替え', 'いく', 'ない', '島', '観光', '死ん', '言う', '旅行', '楽しみ', 'resres', 'travel', '行っ', '非国民', 'られ', '石', '投げ', 'られ', '車', 'つけ', 'られ', '思う', 'reqreq', '星新一', '小説', 'トラベル', '乗じよ', '人々', '観光地', '行く', '現地', 'いる', '防護服', '着', '謎', '職員', '観光客', '収容施設', '連れ', 'いか']\n"
     ]
    }
   ],
   "source": [
    "print(mat[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.,  3., ...,  2., 35.,  8.])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[272, 270, 1368, 774, 4942, 872, 488, 4702, 1656, 4270, 1439, 931, 444, 269, 435, 913, 944, 4593, 7286, 13, 6607, 4281, 4156, 4163, 3770, 6353, 1123, 4954, 6430, 1159, 3325, 1100, 2052, 6065, 2355, 671, 3414, 4166, 430, 6205, 3769, 7513, 943, 3766, 3294, 6304, 6435, 2772, 1655, 7131, 4276, 875, 5978, 4926, 5533, 6562, 37, 6334, 4721, 5014, 6424, 271, 4438, 324, 1151, 877, 6603, 3364, 2685, 2299, 2881, 7002, 3999, 417, 5924, 6608, 1363, 941, 613, 775, 4673, 2227, 3254, 387, 4126, 162, 2847, 1595, 6432, 920, 4567, 5812, 4777, 5913, 1015, 3691, 4247, 5090, 3293, 3475]\n"
     ]
    }
   ],
   "source": [
    "argsort_cnt = [i[0] for i in sorted(enumerate(cnt), key=lambda x:x[1], reverse=True)]\n",
    "print(argsort_cnt[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word occurences...\n",
      "0 resres 3840.0\n",
      "200 無理 69.0\n",
      "400 委託 37.0\n",
      "600 公募 26.0\n",
      "800 都道府県 20.0\n",
      "1000 笑っ 16.0\n",
      "1200 気温 13.0\n",
      "1400 強く 11.0\n",
      "1600 きっかけ 9.0\n",
      "1800 インフル 8.0\n",
      "2000 アリ 7.0\n",
      "2200 豪雨災害 7.0\n",
      "2400 引き続き 6.0\n",
      "2600 ff 5.0\n",
      "2800 土砂降り 5.0\n",
      "3000 老人 5.0\n",
      "3200 アジ 4.0\n",
      "3400 反論 4.0\n",
      "3600 決まり 4.0\n",
      "3800 食材 4.0\n",
      "4000 みつかっ 3.0\n",
      "4200 下がり 3.0\n",
      "4400 国内需要 3.0\n",
      "4600 拙速 3.0\n",
      "4800 癖 3.0\n",
      "5000 返礼 3.0\n",
      "5200 sick 2.0\n",
      "5400 ちゃえ 2.0\n",
      "5600 キャッチフレーズ 2.0\n",
      "5800 ベテラン 2.0\n",
      "6000 会お 2.0\n",
      "6200 合わせる 2.0\n",
      "6400 展示会 2.0\n",
      "6600 掛から 2.0\n",
      "6800 母さん 2.0\n",
      "7000 短期間 2.0\n",
      "7200 行先 2.0\n",
      "7400 選定 2.0\n"
     ]
    }
   ],
   "source": [
    "print(\"word occurences...\")\n",
    "\n",
    "for idx, c in enumerate(argsort_cnt):\n",
    "    if idx % 200 == 0:\n",
    "        print(idx, indices_word[c]  ,cnt[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word occurences [top50] ...\n",
      "0 resres 3840.0\n",
      "1 reqreq 3506.0\n",
      "2 キャンペーン 2211.0\n",
      "3 する 1391.0\n",
      "4 東京 1083.0\n",
      "5 てる 1002.0\n",
      "6 いる 937.0\n",
      "7 旅行 748.0\n",
      "8 トラベル 732.0\n",
      "9 感染 671.0\n",
      "10 コロナ 582.0\n",
      "11 なっ 571.0\n",
      "12 いい 520.0\n",
      "13 req 515.0\n",
      "14 ある 504.0\n",
      "15 ない 501.0\n",
      "16 なる 476.0\n",
      "17 政府 431.0\n",
      "18 除外 424.0\n",
      "19 0人 406.0\n",
      "20 言っ 369.0\n",
      "21 感染者 350.0\n",
      "22 思い 336.0\n",
      "23 思う 332.0\n",
      "24 対象外 294.0\n",
      "25 若者 283.0\n",
      "26 やる 282.0\n",
      "27 東京都 275.0\n",
      "28 行く 269.0\n",
      "29 れる 264.0\n",
      "30 国民 257.0\n",
      "31 やっ 254.0\n",
      "32 ー 247.0\n",
      "33 経済 247.0\n",
      "34 今日 246.0\n",
      "35 ください 243.0\n",
      "36 増え 242.0\n",
      "37 思っ 242.0\n",
      "38 あり 236.0\n",
      "39 考え 225.0\n",
      "40 対象 220.0\n",
      "41 高齢者 217.0\n",
      "42 なり 215.0\n",
      "43 対策 211.0\n",
      "44 団体旅行 210.0\n",
      "45 自粛 207.0\n",
      "46 行っ 207.0\n",
      "47 出 206.0\n",
      "48 トラブル 203.0\n",
      "49 都民 201.0\n"
     ]
    }
   ],
   "source": [
    "print(\"word occurences [top50] ...\")\n",
    "\n",
    "for idx, c in enumerate(argsort_cnt[:50]):\n",
    "\n",
    "    print(idx, indices_word[c]  ,cnt[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_urtext = np.zeros((len(mat),1),dtype=int)\n",
    "for i in range(0,len(mat)):\n",
    "    mat_urtext[i,0] = word_indices[mat[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "with open('dict/word_indices.pickle', 'wb') as f :\n",
    "    pickle.dump(word_indices , f)\n",
    "\n",
    "with open('dict/indices_word.pickle', 'wb') as g :\n",
    "    pickle.dump(indices_word , g)\n",
    "\n",
    "#単語ファイルセーブ\n",
    "with open('dict/words.pickle', 'wb') as h :\n",
    "    pickle.dump(words , h)\n",
    "\n",
    "#コーパスセーブ\n",
    "with open('dict/mat_urtext.pickle', 'wb') as ff :\n",
    "    pickle.dump(mat_urtext , ff)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
