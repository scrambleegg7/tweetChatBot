{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "import re,os\n",
    "import codecs\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "import unicodedata\n",
    "import urllib.request as urllib2\n",
    "import platform\n",
    "\n",
    "import pickle\n",
    "\n",
    "plt = platform.system()\n",
    "\n",
    "#mecab = MeCab.Tagger ('-Ochasen')\n",
    "if plt == \"Linux\":\n",
    "    tagger = MeCab.Tagger('-d /usr/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "if plt == \"Windows\":\n",
    "    tagger = MeCab.Tagger(\"-r C:\\MeCab\\etc\\mecabrc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neologdn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,'./unicode_script_for_python')\n",
    "\n",
    "import unicode_script_map as usm\n",
    "from unicode_script import ScriptType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unicode(text):\n",
    "    removed_str = \"\".join([c for c in text if usm.get_script_type(c) != ScriptType.U_Common])\n",
    "    return removed_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeChar(text):\n",
    "    \n",
    "    text=re.sub(r'[!-~]', \"\", text)#半角記号,数字,英字\n",
    "    text=re.sub(r'[︰-＠]', \"\", text)#全角記号\n",
    "    text=re.sub('\\n', \" \", text)#改行文字\n",
    "    text=re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-…]+', \"\", text)\n",
    "    #text=re.sub('RT', \"\", text)\n",
    "    #text=re.sub('お気に入り', \"\", text)\n",
    "    #text=re.sub('まとめ', \"\", text)\n",
    "    #kigo = '\\\"!\"#$%&'()*+\\,-./;<=>?@[\\]^_`{|}~\"'\n",
    "    \n",
    "    \n",
    "    text = unicodedata.normalize(\"NFKC\", text)  # 全角記号をざっくり半角へ置換（でも不完全）\n",
    "\n",
    "    text = remove_unicode(text)\n",
    "    \n",
    "    kigo = string.punctuation + \"「」、。・\"\n",
    "    collon = str.maketrans( '', '',':')\n",
    "    \n",
    "    kigo = kigo.translate(collon)\n",
    "    table = str.maketrans( '', '',kigo)\n",
    "    text = text.translate(table)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_neologdn(data):\n",
    "\n",
    "    for idx, d in enumerate(data):\n",
    "\n",
    "        prefix = data[idx][0].split(\":\")[0]\n",
    "        sentence = data[idx][0].split(\":\")[1]\n",
    "        \n",
    "        sentence = neologdn.normalize(sentence)\n",
    "        # remove Char\n",
    "        sentence = removeChar(sentence)\n",
    "        data[idx][0] = prefix + \":\" + sentence\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_res_sentence_adjust(data):\n",
    "    \n",
    "    flag = 0\n",
    "    prev_idx = 0\n",
    "    req_idx = []\n",
    "    res_idx = []\n",
    "\n",
    "    print(\"req res sentence adjust routine (replace short with long sentence.)\")\n",
    "\n",
    "    for idx, d in enumerate(data):\n",
    "\n",
    "        prefix = data[idx][0].split(\":\")[0]\n",
    "        sentence = data[idx][0].split(\":\")[1]\n",
    "        \n",
    "        if len(sentence) < 10:\n",
    "            #print(\"short response/ request \")\n",
    "            print(idx,prefix,sentence)\n",
    "            if prefix == \"REQ\":\n",
    "                req_idx.append(idx)\n",
    "            if prefix == \"RES\":\n",
    "                res_idx.append(idx)\n",
    "                    \n",
    "        if prefix == \"REQ\" and idx % 2 == 0:\n",
    "            flag = 1\n",
    "            prev_idx = idx\n",
    "        if prefix == \"RES\" and idx % 2 == 1:\n",
    "            if idx - prev_idx != 1:\n",
    "                print(\"ERROR\", prev_idx, idx)    \n",
    "    \n",
    "    return req_idx, res_idx\n",
    "\n",
    "def req_res_sentence_swap(data, req_idx, res_idx):\n",
    "\n",
    "    for req_i in req_idx:\n",
    "        sentence = data[req_i + 1][0].split(\":\")[1]\n",
    "        #print(sentence)\n",
    "        data[req_i][0] = \"REQ:\" + sentence\n",
    "\n",
    "    for res_i in res_idx:\n",
    "        sentence = data[res_i - 1][0].split(\":\")[1]\n",
    "        data[res_i][0] = \"RES:\" + sentence\n",
    "\n",
    "    return data\n",
    "\n",
    "def removeShortSentence(data):\n",
    "    \n",
    "    flag = 0\n",
    "    prev_idx = 0\n",
    "    req_idx = []\n",
    "    res_idx = []\n",
    "    \n",
    "    new_data = []\n",
    "\n",
    "    print(\"remove short sentence.\")\n",
    "\n",
    "    for idx in range(0, len(data), 2):\n",
    "\n",
    "        prefix = data[idx][0].split(\":\")[0]\n",
    "        sentence_req = data[idx][0].split(\":\")[1]\n",
    "        sentence_res = data[idx][0].split(\":\")[1]\n",
    "        \n",
    "        if len(sentence_req) >= 10:\n",
    "            new_data.append( data[idx]    )\n",
    "            new_data.append( data[idx+1]    )\n",
    "                     \n",
    "            #print(\"short response/ request \")\n",
    "\n",
    "    return  new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows : 2442\n",
      "req res sentence adjust routine (replace short with long sentence.)\n",
      "3 RES 逆にする\n",
      "9 RES 止めて金配れ\n",
      "31 RES せめて関東だよね\n",
      "46 REQ いってらっしゃい\n",
      "69 RES 名古屋そうじゃない\n",
      "81 RES 祖国へキャンペン\n",
      "86 REQ どうかな\n",
      "121 RES 会見しなくとも\n",
      "124 REQ せめて関東だよね\n",
      "132 REQ お題画像に一言\n",
      "156 REQ 私はみんなも️\n",
      "160 REQ \n",
      "161 RES じゃないのか\n",
      "189 RES 仕事ウォ頑張って\n",
      "199 RES してんねうらやま\n",
      "232 REQ \n",
      "251 RES 満員電車\n",
      "333 RES 魔導杯ですわね\n",
      "360 REQ おはようございます\n",
      "374 REQ どうすれば良いんだ\n",
      "387 RES は国外にしろよ\n",
      "403 RES だっったわ\n",
      "405 RES かな︎\n",
      "428 REQ ありがとう\n",
      "491 RES \n",
      "495 RES 昭恵夫人は\n",
      "499 RES \n",
      "542 REQ あその切り口ね鋭い\n",
      "555 RES 手提げカバンで笑\n",
      "568 REQ おはようじゅんや\n",
      "582 REQ 正しい英語は\n",
      "616 REQ 連休の初日は\n",
      "622 REQ 今日もお仕事\n",
      "625 RES お飛ばせ飛ばせ\n",
      "629 RES ですねわかります\n",
      "632 REQ ザギンでシスしたい\n",
      "637 RES 姓が違う\n",
      "650 REQ \n",
      "670 REQ \n",
      "671 RES 私もしたいです\n",
      "723 RES お腹の中\n",
      "727 RES アベよ一一\n",
      "728 REQ 安倍総理に頼んで\n",
      "734 REQ \n",
      "759 RES 松村さん国会\n",
      "778 REQ やめてナウ東京\n",
      "786 REQ 仕事だ́ω\n",
      "807 RES 今なら乗り換えお得\n",
      "822 REQ むくり\n",
      "823 RES むくりキャンペン\n",
      "836 REQ 月日\n",
      "843 RES 一緒に️️️\n",
      "857 RES 家庭教師のトライ\n",
      "884 REQ 僕も毎日仕事ですω\n",
      "902 REQ 分ひま\n",
      "918 REQ 何ででしょうねえ\n",
      "919 RES 文句言うとするぞ\n",
      "929 RES の\n",
      "942 REQ あ今日も雨やしね\n",
      "949 RES マクドした\n",
      "954 REQ \n",
      "955 RES だ気をつけて\n",
      "958 REQ マクドした\n",
      "967 RES しなくてご立派です\n",
      "969 RES してるんだよ\n",
      "971 RES そう可能\n",
      "982 REQ 解脱以前に死ぬ\n",
      "983 RES 三途の川してきて\n",
      "990 REQ エグ\n",
      "992 REQ ほんとだよね\n",
      "1002 REQ ゚内゚ゞ\n",
      "1006 REQ 電車ガラガラ\n",
      "1007 RES みんな\n",
      "1013 RES しな\n",
      "1014 REQ ごろごろぐでっ\n",
      "1019 RES ご近所が静か️\n",
      "1026 REQ ‍️\n",
      "1030 REQ \n",
      "1031 RES っ̆ω̆ς\n",
      "1061 RES これがトラブル\n",
      "1068 REQ 連休初日の乳遊び\n",
      "1069 RES 今すぐそこにしたい\n",
      "1100 REQ 大いに賛成\n",
      "1116 REQ 県外ナンバ多すぎ\n",
      "1120 REQ おはようございます\n",
      "1123 RES なんちゃら\n",
      "1132 REQ アキバ行きたい\n",
      "1138 REQ 間違いないです\n",
      "1143 RES に関してかな\n",
      "1182 REQ はい不要不急\n",
      "1217 RES みんなしちゃったか\n",
      "1218 REQ \n",
      "1219 RES したぃ️笑\n",
      "1223 RES でトラベル\n",
      "1230 REQ 笑笑\n",
      "1231 RES 会いに来るする\n",
      "1246 REQ ️速報️明日の\n",
      "1285 RES してます留守です\n",
      "1314 REQ 日本橋です\n",
      "1328 REQ おいで\n",
      "1329 RES さんちに\n",
      "1333 RES になりましたか\n",
      "1335 RES 魂の\n",
      "1342 REQ 不二家にキャンペン\n",
      "1343 RES です\n",
      "1351 RES さすが讃岐️旅行中\n",
      "1353 RES 皆なのね\n",
      "1356 REQ それな́ω\n",
      "1358 REQ いってもいいの\n",
      "1364 REQ \n",
      "1372 REQ 阿寒湖温泉におるで\n",
      "1373 RES したみ\n",
      "1380 REQ \n",
      "1381 RES お大事に\n",
      "1396 REQ ́Дハァ\n",
      "1399 RES 正にトラブル️\n",
      "1410 REQ \n",
      "1414 REQ \n",
      "1415 RES 飲み会\n",
      "1424 REQ 同意あでも旅が\n",
      "1427 RES もしかして\n",
      "1438 REQ 夏良いよね笑\n",
      "1441 RES キャンペンまじ\n",
      "1442 REQ 逆ならあるよェ\n",
      "1443 RES したいな\n",
      "1466 REQ 当たらないチケット\n",
      "1485 RES の方がええ\n",
      "1494 REQ お頭来ません\n",
      "1497 RES そのまま棺桶\n",
      "1515 RES おおさかキャンペン\n",
      "1534 REQ 裏山裏山裏山\n",
      "1535 RES 車で分\n",
      "1563 RES トラブルです\n",
      "1576 REQ 悩ましいですね\n",
      "1580 REQ いつでもおいでよ\n",
      "1589 RES やね\n",
      "1590 REQ 交通費高いですね笑\n",
      "1604 REQ 肉寿司́\n",
      "1611 RES なんかするから\n",
      "1622 REQ \n",
      "1623 RES くるくる\n",
      "1625 RES \n",
      "1627 RES しちゃいましょ\n",
      "1662 REQ ねほんとそれ́ェ\n",
      "1668 REQ おすごいやん\n",
      "1674 REQ りんくさんもね\n",
      "1688 REQ いよいよ明日\n",
      "1699 RES してんだね\n",
      "1707 RES へんたい\n",
      "1714 REQ 出かけても人混みが\n",
      "1723 RES それはよい\n",
      "1732 REQ へんたい\n",
      "1742 REQ 食べに来ていいよ\n",
      "1757 RES ごめんなさいです\n",
      "1760 REQ ですか\n",
      "1761 RES 元ネタです\n",
      "1781 RES より減税\n",
      "1795 RES 使いましょう笑\n",
      "1813 RES するしかねぇ\n",
      "1820 REQ 人がいない笑\n",
      "1845 RES していいよ️\n",
      "1856 REQ オホツク\n",
      "1882 REQ ソレ切に願います️\n",
      "1886 REQ そうなの\n",
      "1893 RES おそらくでは\n",
      "1902 REQ あいらぁぶいそう\n",
      "1914 REQ 今日の生配信は\n",
      "1916 REQ 町田なの\n",
      "1929 RES 今こそ旅へトラベル\n",
      "1936 REQ ワイがしに行く\n",
      "1937 RES って言われに来るの\n",
      "1968 REQ 北海道行きたい\n",
      "1969 RES お待ちしております\n",
      "1971 RES トラブルでちゅ\n",
      "1982 REQ たまちゃんしてるぜ\n",
      "1983 RES ひちゃんは年中️\n",
      "1999 RES コロナにまけない\n",
      "2002 REQ トラベルです\n",
      "2003 RES ふつかめ\n",
      "2013 RES お呼びですねする\n",
      "2042 REQ ほんとそれ\n",
      "2044 REQ \n",
      "2045 RES キャンペンか\n",
      "2048 REQ キャンペン\n",
      "2059 RES 野田の家にトラベル\n",
      "2063 RES キャンペンなの\n",
      "2065 RES わですか\n",
      "2079 RES こんなデタも\n",
      "2080 REQ ヒント\n",
      "2082 REQ 地獄じゃねえか笑\n",
      "2083 RES バンド名はだな\n",
      "2094 REQ 正直で偉いでも兆\n",
      "2101 RES お頭でお願いします\n",
      "2120 REQ 世界一の街並み́ω\n",
      "2139 RES 留年キャンペンだ\n",
      "2143 RES IIIですね\n",
      "2152 REQ 彩流社\n",
      "2156 REQ キャンペンなう\n",
      "2161 RES そこでですよ\n",
      "2173 RES 不死身お腹\n",
      "2180 REQ ́зいな\n",
      "2184 REQ グッバイ‍️\n",
      "2185 RES ですな\n",
      "2190 REQ \n",
      "2191 RES もつろんでしゅ\n",
      "2206 REQ まじか\n",
      "2211 RES どっかしようや\n",
      "2216 REQ まふかして\n",
      "2222 REQ 結局明日も雨予報\n",
      "2224 REQ やっちゃいますか\n",
      "2242 REQ 落ちたな都知事選\n",
      "2253 RES これではです\n",
      "2254 REQ なの自粛なのどっち\n",
      "2255 RES 家に\n",
      "2262 REQ 麻生もです\n",
      "2280 REQ クソおもろい\n",
      "2296 REQ 遠征の予定\n",
      "2308 REQ レギュラ満タンで\n",
      "2322 REQ レオベルモンドさん\n",
      "2331 RES ですね笑\n",
      "2333 RES か楽しんできて\n",
      "2344 REQ 東京の長崎\n",
      "2345 RES 長崎してきました́\n",
      "2366 REQ あら房総へようこそ\n",
      "2367 RES 県内満喫してる️\n",
      "2388 REQ 日後が恐ろしや️\n",
      "2402 REQ それはヤバい\n",
      "remove short sentence.\n",
      "sanitary checking...\n",
      "req res sentence adjust routine (replace short with long sentence.)\n",
      "2380\n"
     ]
    }
   ],
   "source": [
    "file = \"tweet/tweet_Go To_2020-07-24.txt\"\n",
    "with codecs.open(file, \"r\", \"utf8\", \"ignore\") as f:\n",
    "\n",
    "    \n",
    "    df1 = csv.reader(f)\n",
    "    data = [ v for v in df1]\n",
    "    print('number of rows :', len(data))\n",
    "    \n",
    "    data = sentence_neologdn(data)\n",
    "    req_idx, res_idx = req_res_sentence_adjust(data)\n",
    "    data = req_res_sentence_swap(data, req_idx, res_idx)\n",
    "    data = removeShortSentence(data)\n",
    "    print(\"sanitary checking...\")\n",
    "    _, _ = req_res_sentence_adjust(data)\n",
    "    \n",
    "\n",
    "    \n",
    "    print(len( data ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\\ufeffREQ:この巷の連休仕事で出ることや仕事で移動っていうのはあっても遊びはなし博多に出ることもあるけど人は少ないよやっぱり'],\n",
       " ['RES:なんとかなどなくても普段から旅する時はするしただ今はしてないあとお盆もお寺のお参りと博多くらいしか出ないのは例年のことか笑そういえば天神は月アタマに仕事関連で寄っただけでまったく行ってない'],\n",
       " ['REQ:外出自粛しながらトラベルする方法を知っている方教えて下さい'],\n",
       " ['RES:外出自粛しながらトラベルする方法を知っている方教えて下さい'],\n",
       " ['REQ:ストレス発散法は実家に帰って家族と過ごすことなので最近帰れないからすごくつらい久しぶりにホムシックだようえん'],\n",
       " ['RES:家族もみるくもおなじ気持ちだねもう少し落ち着いてきたら髙木家しておいで'],\n",
       " ['REQ:すいません寝ておりましたこれから仕事なので終わってからはどうですか'],\n",
       " ['RES:お仕事頑張ってください今日から行ってくるので戻りがの夜になっちゃいます'],\n",
       " ['REQ:東京の感染増に危機感を示す一方で家族旅行の自粛不要などと支離滅裂な発言をする西村大臣そもそも感染が急増しているのは東京だけではありません昨日の感染者は東京で人全国では人なので人以上は東京以外で発覚した'],\n",
       " ['RES:東京の感染増に危機感を示す一方で家族旅行の自粛不要などと支離滅裂な発言をする西村大臣そもそも感染が急増しているのは東京だけではありません昨日の感染者は東京で人全国では人なので人以上は東京以外で発覚した'],\n",
       " ['REQ:池江さん自身本当に心から訴えているのだと思いますしかし今オリンピックを行うかどうかは大変申し訳ない事ですがそう言う個人の気持ちを捨象して本当にできるか否かという一点で冷静に判断すべき事です個人の思いを利用するような'],\n",
       " ['RES:りかこさんの気持ちは当然分かりますその上で全世界がコロナ終息に迎えるかですよね息抜きは大切だが政府が判断謝った等やって安易に乗っかる国民が居るようじゃ東京はかなり厳しいね政府国民もっと気を引き締めろ'],\n",
       " ['REQ:はずしたり手洗いしないで食べたりとかだと思うよも止めて欲しかったわ́ωショボン'],\n",
       " ['RES:はないわ東京だけダメとか不公平するほどならやらなきゃいいのに'],\n",
       " ['REQ:身分を隠すってのはあったけど居住県を隠す時代が来るとは'],\n",
       " ['RES:東京以外で安アパト借りてそこを現住所にしてトラベルするって人をでやってましたなんだかなぁです物騒な世の中なのであんまりあけすけに言うのもなんですがでもいつわるのもどうかと'],\n",
       " ['REQ:おはよう御座います昨日は大変なミスをしました送ったつもりですがここにそのままにしてましたアハハまさかのボケ️今朝のモニングユミンはようこそ輝く時間へ'],\n",
       " ['RES:おはようございます私もあるあるです朝から小雨降り出しましたニュスではキャンペンで観光している方々が写っています'],\n",
       " ['REQ:結果こんな感染者急増で行かないで正解かと思いますけど'],\n",
       " ['RES:今行くか行かないかを話しているのではなく都民だとトラベルを利用出来ないことを不公平と言っているのですが'],\n",
       " ['REQ:おはようございます夏休みなんですが長男は休みなだけで毎日時間授業時間は連休も休みなしで塾コロナもですがどこにも出かけられませんのんびりしたい'],\n",
       " ['RES:さんおはようございます各都道府県市町村で夏休み違いがある様ですね職場でも一週間の人も居れば週間の人も働くママには朝のひと時が情報交換の場になりお話聞いてると色々考えさせられます何'],\n",
       " ['REQ:おはようございます゚゚́やった急に今日はお休みになったよパカッ嬉しいなฅ́̆ฅ̀今日お仕事頑張ったもんな\\u200dえへへ今日はダラ'],\n",
       " ['RES:なっちおはよ今日は休みになったんだね️あんまり天気良くないけど思わぬご褒美かな️世の中キャンペンで人が動くからコロナに気をつけないとね'],\n",
       " ['REQ:そりゃもうキャンペンやん️地獄へのほんとにかまってちゃんなんだからすこおおおおおおおお'],\n",
       " ['RES:やねあうちのマヤがマユちゃんのモノマネ全然似てなくて失礼しました'],\n",
       " ['REQ:吉村大阪はウェルカムだって今の状況を考えるとはっきり言って狂ってる'],\n",
       " ['RES:自体英語として変だと言われてるのに今度はウェルカムやれやれ'],\n",
       " ['REQ:しかしながら当たり前にゴツと言ったら目的地はホムかヘルでしょヘブンもありか'],\n",
       " ['RES:は副詞としても使えるので普通はで事足りますなのでやはりと言えば真っ先にですね'],\n",
       " ['REQ:まじで都民を一律に扱うのマジで許さない'],\n",
       " ['RES:まじで都民を一律に扱うのマジで許さない'],\n",
       " ['REQ:今やるべき経済対策として追加の現金給付と消費減税を提案しているが現金給付は相対的に所得の低い方に効く一方で消費減税は一般的な印象と異なり所得の高い人に効果的に効く苦しい生活の下支えをしながら一方で高額商品も含めた消費を回'],\n",
       " ['RES:何回も追加の現金給付と言っていますがなかなか実現していませんね何とか政策を実現させてくださいまた強行でトラベルキャンペンもスタトしましたが経済を回さないといけないのはわかりま'],\n",
       " ['REQ:ぴょりんきょうちゃんゆりっぺちゃんてるさんおはよさん東京から来られたら怖いなぁって話してたらお客さんの子供さんが'],\n",
       " ['RES:姐さんきょうこさんながのぴよりちゃんおはよう́春馬君の事がちゃんの事フラッシュバックで親とし'],\n",
       " ['REQ:おはようございますノってるぜです本日もヨロシクです'],\n",
       " ['RES:おはようございますと蝉が自然は前に進んでますがコロナもしちゃって人間様だけがしてなかなか前に進めない年夏そんな金曜日今宵のサンモルめざしてよろしくです'],\n",
       " ['REQ:トラベルを非難する人間って試行錯誤を否定している人間なのですよね歴史上の英傑も試行錯誤の連続で一回で大成功をしたわけではありません試行錯誤を繰り返して改良改善して良いものを作る'],\n",
       " ['RES:を利用した揚げ足取り検査増加による陽性者増加で煽るカスメディア'],\n",
       " ['REQ:コロナ感染拡大の中キャンペンが始まりましたてニュスで聞こえるアナウンサの声に矛盾しか感じない́おかしいだろ'],\n",
       " ['RES:おはようございますおっしゃる通りですね梅雨明けが待ち遠しいです今日もよろしくお願いします\\u200d️つつがない一日をなおトラベルは怖いので行きませんが'],\n",
       " ['REQ:らびおはどうでも豆知識みかんの皮をレンジでチンするとレンジにこびりついた匂いがとれる個分ぐらいいる連休日目してる私は仕事へリプはらびおはで'],\n",
       " ['RES:同じく仕事へらびおは'],\n",
       " ['REQ:東京以外の新型コロナ陽性率を見てみたら大阪は東京とほとんど変わらないのね愛知に関しては東京大阪より高いとか大都市のある県は単に検査数の違いから感染者数が違ってるだけなのかも'],\n",
       " ['RES:現状の陽性率が変わらない状況を考えるとトラベルやるなら東京のみ除外の意味がわからなくなるな'],\n",
       " ['REQ:さん行ってくるね世間は連休俺はでもコロナ恐いからの方がいい笑さんもお体ご自愛して下さいねくれぐれも無理はなさらないようにね'],\n",
       " ['RES:さん行ってくるね世間は連休俺はでもコロナ恐いからの方がいい笑さんもお体ご自愛して下さいねくれぐれも無理はなさらないようにね'],\n",
       " ['REQ:トラベルで地方に新型コロナ感染が拡大してそれにより不幸にも人以上の死者が発生したら国の政策による死者ということになりますのでこれを強行した政府は責任を取るべきですよね'],\n",
       " ['RES:トラベルと言うのは福島の放射能汚染ゴミを日本各地にばらまいたのと同じ発想ですこの国難を日本人全員が平等に分かち乗り越えようと言う安倍真理教の有難い教えに基ずくものです'],\n",
       " ['REQ:えっまってまってまって世間連休なのわたし明日から連勤だが'],\n",
       " ['RES:連勤キャンペンだった'],\n",
       " ['REQ:社会経済と感染防止の両立という大命題があるこの命題の使い方は本来誤用ですねもはや収拾がつかないほど広まってしまっていますがあえていえば多少の安全より経済を優先すべきであるという命題があるがとか'],\n",
       " ['RES:ついでに感染拡大防止が至上命題となっているも汚名挽回しますと同じくらいヘンですああでもその前にトラベルのほうがもっとストレトにヘンか笑'],\n",
       " ['REQ:おはようございます今日から天気予報の雨マクがしばらく消えてる️梅雨終わったのかなでもまたコロナ感染されてる方増えてるみたいなので手洗いうがいしっかりして気を付けて過ごしてくださいね今日もよろしく'],\n",
       " ['RES:おはようございますもそろそろ梅雨終わってほしぃですよね連休ですがコロナもかなり増えているので外出も最小限にしよかなと思ってますもありいろいろ判断難しぃですが個人の責任でですねまったり過ごします'],\n",
       " ['REQ:パチ屋の行列がすごいよラジ館わきの店で人くらい'],\n",
       " ['RES:多くない他の所ないのかとツッコミたいところだが都内にはちょっとリスクあるせいかパチ屋以外は落ち着いてる感じかな'],\n",
       " ['REQ:シンプルに考えるなら魚津か氷見の漁港付近にある市場や道の駅かな'],\n",
       " ['RES:氷見行こうと思ってましたがてら色々場所見てみます'],\n",
       " ['REQ:これは伝聞だから真偽不明その大学生はどこの誰に問い合わせたのだろうか現在熊本では県内在住者以外のボランティア受付はしていない更に人吉などの災害地に行くのは熊本市内のバスタミナルから無料バスを出している'],\n",
       " ['RES:交通費だけ心配ならよく分からないシステムのキャンペンに拘らずに九州が発行している新幹線も含めて日間乗り放題で福岡熊本なら税込千円という素晴らしい切符がある駅構内の土産物屋食事の割引まである大変ナイスな制度が運用されている'],\n",
       " ['REQ:月日時点の発生状況検査人数',\n",
       "  '444（+334）陽性累計 1',\n",
       "  '371（+4）陰性確認済累計 1',\n",
       "  '181（+6）死亡累計 103（+1）現在患者数 87（-3）【内訳】軽症・中等症 83（-2）重症… '],\n",
       " ['RES:キャバクラクラスタ発生からまだ週間経っていないのとトラベルで道内に入ってくる人がいるのでこれからが正念場ですねどうか爆発的に感染者が増えませんように毎日祈る思いです'],\n",
       " ['REQ:これはちゃんと記録をしておいたほうがいい全国の感染者が過去最多になった翌日からトラベルキャンペンが始まるそして首相はこの事について何のメッセジも出さず官房長官は医療提供体制は逼迫していないといい幹事長にはキャ'],\n",
       " ['RES:人が動けば感染が広がろのは世界の常識重傷者が少ないから経済を回すのではなく少ない時に検査の拡充と検査の拡大だろうやっている事が反対キャンペンはコロナ蔓延キャン'],\n",
       " ['REQ:そんなかわいい顔で言われも'],\n",
       " ['RES:外出自粛しながらトラベルするのってどうすればいいかわからない笑'],\n",
       " ['REQ:え腐るほど余ってるんですが'],\n",
       " ['RES:え腐るほど余ってるんですが'],\n",
       " ['REQ:東京都小池知事感染確認は人日で人以上は初このまま東京の感染者が増えてトラベルやお盆休みで地方にコロナ拡散して医療崩壊したらヤバいでしょ不安と恐怖でがいっぱいになるから癒し系見て落ち'],\n",
       " ['RES:東京や首都近郊の住んでる人がこちらの地方に旅行して来るのが怖いですね最初から政府はキャンペンを辞めておけば良かったのにと思う'],\n",
       " ['REQ:今宵はさんいなかったよ'],\n",
       " ['RES:不要不急のしちゃったかな️'],\n",
       " ['REQ:足りない強盗トラブルには兆円持続化給付金事業で兆円観光業だけに持続化給付金の分の予算このキャンペンの対象は基本的には宿泊業者用宿泊費に関しては単'],\n",
       " ['RES:どうしても補償したいと言うのなら持続化給付金に上乗せって形にすれば良い当然他業種からは批難を浴びるでしょうがそもそもの予算は医療に回すべきである病院が廃業'],\n",
       " ['REQ:ライブドアでは無いことを知っての発言ですよね笑'],\n",
       " ['RES:いや今の内閣なら普通にやりかねないなって笑の件やマスク二枚の事もあってそんな馬鹿なことを国がする訳ないじゃんと誰も言わない状況流石に笑ったので便乗しました'],\n",
       " ['REQ:うちの旦那さんもよく言ってるけど専門家とか政府の人とか数だけで判断しないでちゃんと現場に行って状況を見てから判断してくれって現場は大変よねほんとに頭が下がるよがんばってね'],\n",
       " ['RES:本当に温度差を感じていますキャンペンも決して反対ではないのですが今なのかなって疑問符をくっつけながら毎日お仕事しています'],\n",
       " ['REQ:在日三世生きるか死ぬかの瀬戸際悲壮なつぶやきをしている人が沢山資産を凍結して収容所に送られる'],\n",
       " ['RES:在日三世生きるか死ぬかの瀬戸際悲壮なつぶやきをしている人が沢山資産を凍結して収容所に送られる'],\n",
       " ['REQ:そう️家から出るな️'],\n",
       " ['RES:外出自粛しながらトラベルするなんてムリですにゃ笑'],\n",
       " ['REQ:国と都が直前に真逆のことを言ってそれで平然としている子供じみたケンカをしている場合なのか国民は都民は一体どう行動すればいいのか毎日毎日事態を混乱させるだけの政治あきれ果てるだがそれでは済まないこんなことでは'],\n",
       " ['RES:これまでの安倍政権の消極達なコロナ対策と天下の愚策キャンペンの強行を見れば都民の取るべき選択がどちらかなど火を見るより明らかです少なくとも東京都は検査を拡大して感染者を隔離して感染'],\n",
       " ['REQ:我が娘たちの餌食できないのでお家でしたいです金魚すくいかわたあめ希望ですÜ'],\n",
       " ['RES:我が娘たちの餌食できないのでお家でしたいです金魚すくいかわたあめ希望ですÜ'],\n",
       " ['REQ:えんちょ先生みなさまおはようございます️感染者が確実に増えています引きこもり生活からやり直しですね今日も宜しくお願いします️'],\n",
       " ['RES:中島市兵衛さんおはようございます️️感染者増えてるのにキャンペンで各地に出かけてる人のなんと多いことか自粛出来る人は自粛で自分を守るしかないですね本日もよろしくお願い致します️'],\n",
       " ['REQ:家の近くの研修センタでクラスタ感染д何処で何がおこるか自分の出来ることもう一度見直して自己防衛'],\n",
       " ['RES:心音さん身近でクラスタとか怖いねこんなに数やら増えてるのに軽症だ無症状だって強調してるけどなんちゃら後が怖いですくれぐれも気をつけて下さいね'],\n",
       " ['REQ:祝日土日は休みですよ️'],\n",
       " ['RES:そうなんだ良かったねキャンペンは何処か行くのかい'],\n",
       " ['REQ:ガス室に送られたりナタを持って襲ってこられたり連れ出され殺されたりの妄想から解放されるために今すぐ荷物をまとめて祖国にお帰りになればよろしいさすればその様な妄想は直ぐに無くなる事でしょう祖国'],\n",
       " ['RES:祖国キャンペン素晴らしいまずは生活保護受給の外国人に無料航空チケット進呈の再入国禁止'],\n",
       " ['REQ:安倍総理批判に繋げたい'],\n",
       " ['RES:で感染者が増えてほしいと心から思ってるのはマスゴミと野党の連中です'],\n",
       " ['REQ:がコロナウイルスワクチンは年内の接種は無理だとの見解を以前森会長も開催にはワクチンがと言っていたがこれで来年の東京オリンピックはほぼ中止の決断をしなくてはいけなくなったと思う日本の感染者も広まる一方でアスリトの夢のオリンピックですが命が優先です'],\n",
       " ['RES:今の政府は献金してくれる人や組織票をまとめてくれる人後援会等の支持者以外の命はどうでも良いのですその証拠に感染者が増大する中で平然とキャンペン行うのです']]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reqresCounter(data):\n",
    "\n",
    "    prefix_counts = []\n",
    "\n",
    "    prefix_req_cnt = 0\n",
    "    for i in range(len(data)):\n",
    "        prefix = data[i][0].split(\":\")[0]\n",
    "        prefix_counts.append(prefix)\n",
    "        \n",
    "        if (re.search(\"REQ:\", data[i][0])):\n",
    "            prefix_req_cnt += 1\n",
    "             \n",
    "\n",
    "    np_prefix = np.array( prefix_counts )\n",
    "    print(len(np_prefix))\n",
    "\n",
    "    prefix_index = np.ones(len(np_prefix))\n",
    "\n",
    "    resres = prefix_index[np_prefix == \"RES\"]\n",
    "    reqreq = prefix_index[np_prefix == \"REQ\"]\n",
    "\n",
    "    print(\"RESRES counting --> \",np.sum( resres ))\n",
    "    print(\"REQREQ counting --> \",np.sum( reqreq ))\n",
    "    \n",
    "    print(prefix_req_cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2380\n",
      "RESRES counting -->  1190.0\n",
      "REQREQ counting -->  1189.0\n",
      "1190\n"
     ]
    }
   ],
   "source": [
    "reqresCounter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nan(x):\n",
    "    return (x != x)\n",
    "\n",
    "def lower_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "def normalize_number(text):\n",
    "    # 連続した数字を0で置換\n",
    "\n",
    "    replaced_text = re.sub(r'\\d+', '0', text)\n",
    "    return replaced_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sloth_stop_words():\n",
    "    slothlib_path = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "    slothlib_file = urllib2.urlopen(slothlib_path)\n",
    "    slothlib_stopwords = [line.decode(\"utf-8\").strip() for line in slothlib_file]\n",
    "    slothlib_stopwords = [ss for ss in slothlib_stopwords if not ss==u'']\n",
    "    \n",
    "    #print(slothlib_stopwords)\n",
    "    return slothlib_stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = sloth_stop_words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-eeac1eacbabc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcount_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_pattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'(?u)\\\\b\\\\w+\\\\b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfeature_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#vocabulary = count_vectorizer.get_feature_names()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#sentence_extracted_words = [p for p in split_words if p in vocabulary ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1058\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m    990\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "split_words = ['今']\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words=stop_words,token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\n",
    "feature_vectors = count_vectorizer.fit_transform(split_words)\n",
    "#vocabulary = count_vectorizer.get_feature_names()\n",
    "#sentence_extracted_words = [p for p in split_words if p in vocabulary ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['いわ', 'れる', '可能性', 'あり']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_extracted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['あり', 'いわ', 'れる', '可能性']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  \n",
      "sentence line counts -->  0\n",
      "original sentence :  REQ:いつもの見出ししか読まない人らが万のところまでたどり着けるわけがない\n",
      "word counts by MeCab : 10\n",
      "extracted words :  ['reqreq', '見出し', '読ま', '人', 'ら', '万', 'ところ', 'たどり着ける', 'わけ', 'ない']\n",
      "--  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  \n",
      "sentence line counts -->  1000\n",
      "original sentence :  REQ:イベント緩和当面延期へコロナ感染再拡大で政府産経新聞\n",
      "word counts by MeCab : 16812\n",
      "extracted words :  ['reqreq', 'イベント', '緩和', '当面', '延期', 'コロナ感染', '拡大', '政府', '産経新聞']\n",
      "reqreq counter 530\n",
      "resres counter 530\n",
      "exclude Stop words\n",
      "parts printing (top100)...\n",
      "['reqreq', '見出し', '読ま', 'ら', 'たどり着ける', 'ない', 'resres', 'ニカイガ', 'とこ', '読ん', 'ω', 'キャンペン', '推進', '派', '階', '頑張れ', '階', '地元', '和歌山', '旅行', '行こ', 'reqreq', '今日も一日', '外来', 'リハビリ', '一日', '終了', 'これだけ', '親指', '付け根', '解し', '分の', '程度', '明日', 'やっ', 'もらい', 'いいね', '返し', '出来', '申し訳', 'フォロバッ', 'resres', '今日', '連日', 'リハビリ', '翌日', '支障', 'きたさ', '注意', 'し', 'ください', '美ヶ原', '高原', '綺麗', '明日', 'はじまる', '新型', 'コ', 'reqreq', '人吉市', '避難所', '火の国', '湯', '名付け', 'られ', '自衛隊', '入浴', '支援', '行っ', 'い', '発', '災', '一週間', '余り', '被災者', '皆さん', '救助活動', '災害廃棄物', '搬出', '支援', '生活支援', '全力', '当たっ', 'いる', '彼ら', '誇り', '思い', 'resres', '二階', '切り捨て', 'なさい', '足', '引っ張る', 'ゴミ', 'ネット', '使いこなせ', '老害', '階', '己', '評判']\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "reqreq_counts = []\n",
    "resres_counts = []\n",
    "parts = []\n",
    "re_hiragana = re.compile(r'[あ-んA-Za-z0-9]$')\n",
    "\n",
    "for i in range(len(data)) :\n",
    "\n",
    "                \n",
    "    try:\n",
    "        #after_data =omitChar(data[i][0])\n",
    "        result = tagger.parse(data[i][0])\n",
    "        df_analyzed = pd.read_csv(StringIO(result), delimiter='\\t', names=['単語'] )\n",
    "    except:\n",
    "        print(\"- \" * 20)\n",
    "        print()\n",
    "        print(\"*********          error           ********\")\n",
    "        #print(data[i][0])\n",
    "        print(\"BEFORE--> \",normalize_neologd(data[i][0]))\n",
    "        after_data =omitChar(data[i][0])\n",
    "        print(\"AFTER --> \", after_data)\n",
    "        #result = tagger.parse( after_data )\n",
    "        #df_analyzed = pd.read_csv(StringIO(result), delimiter='\\t', names=['単語'] )\n",
    "\n",
    "        continue\n",
    "\n",
    "    sentence_extracted_words = []\n",
    "    for idx, w in enumerate(df_analyzed[\"単語\"]):\n",
    "        #print(w.split(\",\")[6])\n",
    "        if w != \"EOS\" and not is_nan(w):\n",
    "\n",
    "            #word = w.split(\",\")[6]\n",
    "            word = df_analyzed.index[idx]\n",
    "            type = w.split(\",\")[0]\n",
    "            if not type in (\"名詞\",\"動詞\",\"形容詞\"):\n",
    "                continue\n",
    "\n",
    "            if word == \"REQ\" and df_analyzed.index[idx+1] == \":\":\n",
    "                word = \"REQREQ\"\n",
    "                reqreq_counts.append(word)\n",
    "            if word == \"RES\" and df_analyzed.index[idx+1] == \":\":\n",
    "                word = \"RESRES\"\n",
    "                resres_counts.append(word)\n",
    "\n",
    "            word = normalize_number(word)\n",
    "            word = lower_text(word)\n",
    "            #print(\"words...%d %s \" % (idx, word)  )\n",
    "            if word in [\"go\", \"to\", \"goto\"]:\n",
    "                continue\n",
    "            parts.append( word )\n",
    "            sentence_extracted_words.append(word)\n",
    "\n",
    "\n",
    "        else :\n",
    "            #print(i, ' skip')\n",
    "            #skip_counts += 1\n",
    "            continue\n",
    "\n",
    "    if i % 1000 == 0 :\n",
    "        print(\"--  \" *20)\n",
    "        #print(df_analyzed)\n",
    "        print(\"sentence line counts --> \", (i)  )\n",
    "        print(\"original sentence : \", data[i][0])\n",
    "        print(\"word counts by MeCab :\", len(parts))\n",
    "        print(\"extracted words : \", sentence_extracted_words)\n",
    "\n",
    "print(\"reqreq counter\", len(reqreq_counts))\n",
    "print(\"resres counter\", len(resres_counts))\n",
    "\n",
    "print(\"exclude Stop words\")\n",
    "count_vectorizer = CountVectorizer(stop_words=stop_words,token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\n",
    "feature_vectors = count_vectorizer.fit_transform(parts)\n",
    "vocabulary = count_vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "parts = [p for p in parts if p in vocabulary ]\n",
    "print(\"parts printing (top100)...\")\n",
    "print(parts[:100])\n",
    "print(\"====\")\n",
    "\n",
    "new_parts = []\n",
    "re_hiragana = re.compile(r'[あ-んA-Za-z0-9]$')\n",
    "hiragana_status = [ re_hiragana.fullmatch(w) for w in parts]\n",
    "for idx, status in enumerate(hiragana_status):\n",
    "    if not status:\n",
    "        new_parts.append(parts[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wordFreqChecker(data):\n",
    "\n",
    "    frequency = defaultdict(int)\n",
    "\n",
    "    for w in data:\n",
    "        frequency[w] += 1\n",
    "\n",
    "    dataset = [w for w in data if frequency[w] > 1]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reqreq',\n",
       " '見出し',\n",
       " '読ま',\n",
       " 'たどり着ける',\n",
       " 'ない',\n",
       " 'resres',\n",
       " 'ニカイガ',\n",
       " 'とこ',\n",
       " '読ん',\n",
       " 'ω',\n",
       " 'キャンペン',\n",
       " '推進',\n",
       " '派',\n",
       " '階',\n",
       " '頑張れ',\n",
       " '階',\n",
       " '地元',\n",
       " '和歌山',\n",
       " '旅行',\n",
       " '行こ',\n",
       " 'reqreq',\n",
       " '今日も一日',\n",
       " '外来',\n",
       " 'リハビリ',\n",
       " '一日',\n",
       " '終了',\n",
       " 'これだけ',\n",
       " '親指',\n",
       " '付け根',\n",
       " '解し',\n",
       " '分の',\n",
       " '程度',\n",
       " '明日',\n",
       " 'やっ',\n",
       " 'もらい',\n",
       " 'いいね',\n",
       " '返し',\n",
       " '出来',\n",
       " '申し訳',\n",
       " 'フォロバッ',\n",
       " 'resres',\n",
       " '今日',\n",
       " '連日',\n",
       " 'リハビリ',\n",
       " '翌日',\n",
       " '支障',\n",
       " 'きたさ',\n",
       " '注意',\n",
       " 'ください',\n",
       " '美ヶ原',\n",
       " '高原',\n",
       " '綺麗',\n",
       " '明日',\n",
       " 'はじまる',\n",
       " '新型',\n",
       " 'コ',\n",
       " 'reqreq',\n",
       " '人吉市',\n",
       " '避難所',\n",
       " '火の国',\n",
       " '湯',\n",
       " '名付け',\n",
       " 'られ',\n",
       " '自衛隊',\n",
       " '入浴',\n",
       " '支援',\n",
       " '行っ',\n",
       " '発',\n",
       " '災',\n",
       " '一週間',\n",
       " '余り',\n",
       " '被災者',\n",
       " '皆さん',\n",
       " '救助活動',\n",
       " '災害廃棄物',\n",
       " '搬出',\n",
       " '支援',\n",
       " '生活支援',\n",
       " '全力',\n",
       " '当たっ',\n",
       " 'いる',\n",
       " '彼ら',\n",
       " '誇り',\n",
       " '思い',\n",
       " 'resres',\n",
       " '二階',\n",
       " '切り捨て',\n",
       " 'なさい',\n",
       " '足',\n",
       " '引っ張る',\n",
       " 'ゴミ',\n",
       " 'ネット',\n",
       " '使いこなせ',\n",
       " '老害',\n",
       " '階',\n",
       " '己',\n",
       " '評判',\n",
       " '理解',\n",
       " '文春',\n",
       " '報道',\n",
       " 'キャンペン',\n",
       " '受託',\n",
       " '団体',\n",
       " '二階',\n",
       " '幹事長',\n",
       " '献金',\n",
       " 'reqreq',\n",
       " 'てる',\n",
       " '怖い',\n",
       " '福岡',\n",
       " '身近',\n",
       " 'すぎ',\n",
       " '他県',\n",
       " 'resres',\n",
       " '住ん',\n",
       " 'でる',\n",
       " '罹患者',\n",
       " '出て',\n",
       " '来',\n",
       " '怖い',\n",
       " 'どころ',\n",
       " 'reqreq',\n",
       " '農林',\n",
       " '漁業者',\n",
       " '中小',\n",
       " '小規模',\n",
       " '事業者',\n",
       " '皆さん',\n",
       " 'コロナウイルス',\n",
       " '影響',\n",
       " '受ける',\n",
       " '災害',\n",
       " '発生',\n",
       " '事業',\n",
       " '再開',\n",
       " '気力',\n",
       " '失い',\n",
       " 'かね',\n",
       " '厳しい',\n",
       " '状況',\n",
       " 'ある',\n",
       " '声',\n",
       " 'うかがい',\n",
       " 'resres',\n",
       " '二階',\n",
       " '切り捨て',\n",
       " 'なさい',\n",
       " '足',\n",
       " '引っ張る',\n",
       " 'ゴミ',\n",
       " 'ネット',\n",
       " '使いこなせ',\n",
       " '老害',\n",
       " '階',\n",
       " '己',\n",
       " '評判',\n",
       " '理解',\n",
       " '文春',\n",
       " '報道',\n",
       " 'キャンペン',\n",
       " '受託',\n",
       " '団体',\n",
       " '二階',\n",
       " '幹事長',\n",
       " '献金',\n",
       " 'reqreq',\n",
       " '政治家',\n",
       " '一人',\n",
       " '国民',\n",
       " '幸せ',\n",
       " '平和ボケ',\n",
       " '政治家',\n",
       " '失敗',\n",
       " '説明',\n",
       " '責任',\n",
       " '取ら',\n",
       " '利権',\n",
       " '受け取る',\n",
       " '穀潰し',\n",
       " 'resres',\n",
       " '政治家',\n",
       " '一人',\n",
       " '国民',\n",
       " '幸せ',\n",
       " '平和ボケ',\n",
       " '政治家',\n",
       " '失敗',\n",
       " '説明',\n",
       " '責任',\n",
       " '取ら',\n",
       " '利権',\n",
       " '受け取る',\n",
       " '穀潰し',\n",
       " 'reqreq',\n",
       " 'ア',\n",
       " '内閣',\n",
       " '失政',\n",
       " '責任',\n",
       " '一派',\n",
       " '転嫁']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
